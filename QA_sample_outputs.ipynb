{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roger Mei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.summarization.bm25 import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from rake_nltk import Rake\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# initializing stop words \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "path_13 = './2013'\n",
    "path_14 = './2014'\n",
    "path_train = './training/'\n",
    "\n",
    "# load training set\n",
    "text_2013 = []\n",
    "text_2014 = []\n",
    "\n",
    "# reading 2013 texts\n",
    "for file in os.listdir(path_13):\n",
    "    text_2013.append(open(os.path.join(path_13,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "    \n",
    "# reading 2014 texts\n",
    "for file in os.listdir(path_14):\n",
    "    text_2014.append(open(os.path.join(path_14,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "    \n",
    "# Combining both 2013 and 2014 texts into one list\n",
    "all_text = text_2013 + text_2014\n",
    "\n",
    "# word tokenizing all documents in corpus:\n",
    "def tokenize_texts(all_text):\n",
    "    '''\n",
    "    input:\n",
    "        all_text - list of str: list of all documents\n",
    "    return:\n",
    "        tokenized_texts - list of list of str: list of word tokenized documents\n",
    "    '''\n",
    "    tokenized_texts = []\n",
    "    for text in all_text:\n",
    "        tokenized_texts.append(word_tokenize(text))\n",
    "    return tokenized_texts\n",
    "        \n",
    "tokenized_texts = tokenize_texts(all_text)\n",
    "print(len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating document retrieval hashmap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('6.5', [0, 1, 2, 3, 8, 16, 17, 21, 22, 23, 24, 25, 26, 28, 29, 37, 38, 39, 40, 41, 43, 45, 47, 48, 50, 54, 57, 58, 60, 64, 65, 66, 68, 71, 72, 76, 82, 83, 86, 87, 88, 94, 95, 98, 107, 108, 109, 111, 116, 118, 119, 126, 129, 131, 132, 134, 136, 137, 144, 145, 153, 155, 156, 158, 159, 160, 161, 162, 163, 168, 170, 172, 174, 175, 176, 177, 180, 181, 183, 197, 198, 200, 201, 203, 208, 209, 210, 216, 218, 220, 221, 222, 224, 226, 227, 228, 233, 235, 238, 239, 243, 244, 248, 255, 256, 261, 264, 266, 267, 270, 273, 281, 285, 291, 292, 294, 297, 299, 303, 304, 305, 306, 307, 309, 312, 314, 318, 327, 328, 333, 334, 335, 337, 338, 340, 341, 343, 344, 353, 355, 359, 363, 371, 376, 379, 381, 383, 384, 385, 386, 390, 393, 400, 404, 408, 415, 416, 418, 423, 431, 433, 437, 438, 440, 443, 451, 453, 458, 459, 463, 466, 468, 469, 470, 471, 473, 481, 483, 485, 489, 494, 495, 496, 498, 502, 504, 505, 509, 511, 512, 513, 517, 518, 523, 528, 531, 532, 536, 538, 541, 542, 544, 545, 548, 551, 557, 560, 562, 563, 564, 565, 568, 573, 574, 575, 576, 577, 578, 581, 584, 586, 587, 597, 598, 599, 601, 602, 604, 607, 608, 612, 614, 616, 621, 622, 624, 626, 631, 632, 645, 647, 648, 649, 650, 655, 659, 660, 665, 667, 668, 671, 672, 673, 675, 676, 683, 684, 687, 692, 693, 694, 697, 698, 699, 701, 703, 706, 716, 718, 721, 722, 725, 726])\n",
      "('formula', [0, 1, 4, 6, 7, 13, 20, 26, 30, 32, 36, 38, 40, 47, 49, 50, 53, 66, 73, 82, 83, 91, 92, 96, 98, 101, 105, 109, 126, 130, 131, 132, 133, 143, 144, 148, 151, 156, 158, 164, 175, 178, 186, 194, 200, 208, 222, 223, 227, 229, 230, 231, 233, 238, 258, 261, 266, 269, 271, 273, 274, 279, 285, 290, 291, 300, 303, 306, 307, 308, 313, 315, 318, 330, 331, 332, 335, 337, 342, 344, 346, 348, 355, 358, 360, 365, 368, 371, 374, 375, 376, 378, 379, 380, 382, 384, 386, 396, 399, 400, 405, 417, 418, 419, 433, 440, 444, 445, 450, 454, 459, 462, 475, 478, 481, 482, 488, 490, 492, 493, 496, 500, 501, 503, 504, 505, 514, 516, 528, 529, 532, 534, 539, 542, 547, 548, 552, 557, 558, 561, 564, 565, 566, 572, 577, 584, 586, 590, 597, 598, 599, 603, 604, 608, 611, 614, 620, 636, 638, 639, 645, 647, 648, 653, 657, 658, 660, 668, 669, 670, 675, 677, 681, 682, 685, 687, 689, 690, 691, 692, 693, 696, 704, 705, 710, 712, 717, 718])\n",
      "('h.', [0, 2, 14, 24, 27, 49, 60, 63, 68, 74, 88, 95, 105, 109, 114, 120, 123, 126, 130, 133, 144, 148, 153, 157, 165, 168, 173, 184, 202, 211, 218, 233, 238, 257, 269, 271, 276, 278, 281, 284, 300, 304, 306, 313, 336, 339, 344, 348, 354, 360, 368, 369, 379, 387, 392, 410, 416, 423, 442, 443, 451, 454, 468, 473, 477, 479, 486, 495, 508, 513, 523, 525, 531, 535, 548, 551, 562, 565, 569, 581, 582, 583, 584, 587, 589, 607, 610, 619, 631, 632, 633, 644, 651, 657, 659, 672, 673, 679, 683, 689, 692, 700, 710, 716, 717, 721, 724])\n"
     ]
    }
   ],
   "source": [
    "# creating document retrieval hashmap\n",
    "def create_hashmap(all_text):    \n",
    "    hashmap = {}\n",
    "    '''\n",
    "    input:\n",
    "        all_text - list of str: list of all documents\n",
    "    return:\n",
    "        hashmap - dict: dictionary of word, document index pairs that map a word to documents containing that word\n",
    "    '''\n",
    "    for i in range(len(all_text)):\n",
    "        words_in_doc = set(word_tokenize(all_text[i].lower()))\n",
    "        words_in_doc = [word for word in words_in_doc if word not in stop_words]\n",
    "        for word in words_in_doc:\n",
    "            if not hashmap.get(word):                \n",
    "                hashmap[word] = [i]\n",
    "            else:\n",
    "                hashmap[word].append(i)\n",
    "    return hashmap\n",
    "\n",
    "# previewing data in hashmap     \n",
    "hashmap = create_hashmap(all_text)\n",
    "iterator = iter(hashmap.items())\n",
    "for i in range(3):\n",
    "    print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question type classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•Questions containing “Who” or “Whom” are taken to be of type **Person**.  \n",
    "•Questions starting with “Where” are related to type  **Location**.  \n",
    "•Questions starting with “When” , “What time” or “What date” are related to type **Date/Time**.   \n",
    "•Questions starting with “How few”, “How great”, “How little”, “How many” or “How much” are taken to be of type **Quantity**.  \n",
    "•Questions starting with \"Which company/companies\" are of type **Organization**.  \n",
    "•Questions starting with \"What\" without any other information are of type **Noun**.\n",
    "\n",
    "After document retrieval, only sentences that include the specified named entities will be considered as answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question type classification\n",
    "def question_class(q):\n",
    "    '''\n",
    "    input:\n",
    "        q - str: question to be asked\n",
    "    return:\n",
    "        classification - list of str: classifications of types of answers expected as a spacy named entity\n",
    "    '''\n",
    "    q = word_tokenize(q.lower())\n",
    "    classification = []\n",
    "    if (\"who\" == q[0]) or (\"whom\" == q[0]):\n",
    "        classification = ['PERSON']\n",
    "    elif (\"where\") == q[0]:\n",
    "        classification = ['LOC','GPE']\n",
    "    elif (\"when\" == q[0]) or (\"what\" == q[0] and q[1] in ['date','time']):\n",
    "        classification = [\"TIME\",\"DATE\"]\n",
    "    elif (\"how\" == q[0]) and (q[1] in ['few','great','little','many','much']):\n",
    "        classification = ['QUANTITY','MONEY','PERCENT']    \n",
    "    elif (\"which\" == q[0]) and (q[1] in ['company','companies']):\n",
    "        classification = ['ORG']\n",
    "    elif (\"what\" == q[0]) and (q[1] in ['%','percent','percents','percentage','percentages']):\n",
    "        classification = ['PERCENT']\n",
    "    # answers are nouns\n",
    "    elif \"what\" == q[0]:\n",
    "        classification = ['NN']\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['QUANTITY', 'MONEY', 'PERCENT']\n",
      "['PERCENT']\n"
     ]
    }
   ],
   "source": [
    "print(question_class(\"How many points did the S&P decrease by?\"))\n",
    "print(question_class(\"What percentage of drop or increase is associated with this property?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting keywords from question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>selected keywords</u>  \n",
    "•All non-stop words of a quoted expression    \n",
    "•All name entities, recognized as proper nouns    \n",
    "•All noun and their adjectival modifiers    \n",
    "•For example: “What is the name of the \"female\" counterpart to El Nino, which results in cooling temperatures and very dry weather?”      \n",
    " --Keywords : name/ female/ counterpart/ El Nino / cooling/ temperatures/ dry/ weather/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Special cases for business type questions:</u>  \n",
    "•“organization” is dropped and replaced as most organizations mentioned in the documents may contain “Co.”, “Ltd.” but not the word “organization” itself  \n",
    "•“people” should be kept and expanded to include “residents” or “populations” or “citizens”  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extract(q):\n",
    "    '''\n",
    "    input:\n",
    "        q - str: question to be asked\n",
    "    return:\n",
    "        filtered_result - list of str: keywords found in question\n",
    "    '''\n",
    "    result = []\n",
    "    \n",
    "    # extract keywords\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(q)\n",
    "    keywords = set(r.get_ranked_phrases())\n",
    "    # if more than one word in a string, split the string\n",
    "    for kw in keywords:\n",
    "        if \" \" in kw:\n",
    "            split_kw = kw.split()\n",
    "            for word in split_kw:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(kw)\n",
    "    \n",
    "    # remove verbs\n",
    "    filtered_result = []\n",
    "    tags = nltk.pos_tag(word_tokenize(q))\n",
    "    for tag in tags:\n",
    "        if 'VB' not in tag[1] or tag[1] == 'VBG':\n",
    "            if tag[0].lower() in result:\n",
    "                filtered_result.append(tag[0].lower())\n",
    "     \n",
    "    return filtered_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos tags:\n",
      "[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('name', 'NN'), ('of', 'IN'), ('the', 'DT'), ('``', '``'), ('female', 'JJ'), (\"''\", \"''\"), ('counterpart', 'NN'), ('to', 'TO'), ('El', 'NNP'), ('Nino', 'NNP'), (',', ','), ('which', 'WDT'), ('results', 'NNS'), ('in', 'IN'), ('cooling', 'VBG'), ('temperatures', 'NNS'), ('and', 'CC'), ('very', 'RB'), ('dry', 'JJ'), ('weather', 'NN'), ('?', '.')]\n",
      "\n",
      "keywords:\n",
      "['name', 'female', 'counterpart', 'El', 'Nino', 'results', 'cooling', 'temperatures', 'dry', 'weather']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['companies', 'bankrupt', 'February', '2012']"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pos tags:\")\n",
    "print(nltk.pos_tag(word_tokenize(\"What is the name of the \\\"female\\\" counterpart to El Nino, which results in cooling temperatures and very dry weather?\")))\n",
    "# pos_tag incorrectly tags \"results\" as a plural noun\n",
    "\n",
    "print(\"\\nkeywords:\")\n",
    "extracted = keyword_extract(\"What is the name of the \\\"female\\\" counterpart to El Nino, which results in cooling temperatures and very dry weather?\")\n",
    "print(extracted)\n",
    "\n",
    "keyword_extract(\"What companies went bankrupt in February 2012?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter to select only documents that contain extracted keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_select(keywords,hashmap):\n",
    "    '''\n",
    "    input:\n",
    "        keywords - list of str: keywords found in question\n",
    "        hashmap - dict: dictionary of word, document index pairs that map a word to documents containing that word\n",
    "    return:\n",
    "        docs - list of int: indices of documents to be used for answer\n",
    "    '''\n",
    "    docs = []\n",
    "    for k in keywords:\n",
    "        try:\n",
    "            docs += hashmap[k]\n",
    "        except:\n",
    "            continue\n",
    "    return list(set(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tesla', 'ceo']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 136, 137, 138, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 317, 318, 319, 321, 322, 323, 324, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 342, 343, 344, 345, 346, 347, 348, 350, 351, 352, 353, 354, 355, 358, 359, 360, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 426, 428, 430, 431, 432, 433, 434, 435, 436, 437, 438, 440, 441, 442, 443, 444, 445, 446, 447, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 523, 524, 525, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 679, 680, 681, 682, 683, 684, 685, 686, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729]\n"
     ]
    }
   ],
   "source": [
    "keywords = keyword_extract(\"Who is the CEO of Tesla?\")\n",
    "print(keywords)\n",
    "print(doc_select(keywords,hashmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okapi BM25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On selected documents, select 5 documents with best okapi scores based on the BM25 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"okapi.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_scoring(q,tokenized_texts,hashmap):\n",
    "    '''\n",
    "    input:\n",
    "        q - str: question to score each document against\n",
    "        tokenized_texts - list of list of str: list of word tokenized documents to be scored\n",
    "        hashmap - dict: dictionary of word, document index pairs that map a word to documents containing that word\n",
    "    return:\n",
    "        best_docs - list of int: list of indices of the 5 most relevant document to the question based on okapi scores\n",
    "    '''\n",
    "    # setup\n",
    "    keywords = keyword_extract(q)\n",
    "    docs = doc_select(keywords,hashmap)\n",
    "    filtered_texts = [tokenized_texts[i] for i in docs]\n",
    "    # scoring\n",
    "    bm25 = BM25(filtered_texts)\n",
    "    query = word_tokenize(q)\n",
    "    scores = bm25.get_scores(query)\n",
    "    best_docs = []\n",
    "    \n",
    "    for idx in list(np.argsort(scores)[-5:]):\n",
    "        best_docs.append(docs[idx])\n",
    "\n",
    "    return best_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 675, 107, 697, 4]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okapi_scoring(\"Who is the CEO of company Tesla?\",tokenized_texts,hashmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 5 best documents, choose the most relevant sentence based on cosine similarity to question asked\n",
    "(returns both answers and sentence for debugging purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_select(q,docs):\n",
    "    '''\n",
    "    input:\n",
    "        q - str: question to score each sentence against\n",
    "        docs - list of str: list of documents to select sentence from\n",
    "    return:\n",
    "        answers - list of str: most relevant answers to query\n",
    " \n",
    "    '''\n",
    "    best_sent = \"\"\n",
    "    best_cs = 0\n",
    "    \n",
    "    q_type = question_class(q)\n",
    "    for doc in docs:\n",
    "        # sentence tokenize document\n",
    "        sentences = sent_tokenize(doc)\n",
    "        filtered_sentences = []\n",
    "\n",
    "        # keep only sentences with wanted entities\n",
    "        nlp = spacy.load('en')\n",
    "        if len(q_type) and q_type[0] != 'NN':\n",
    "            for sent in sentences:\n",
    "                doc_obj = nlp(sent)\n",
    "                for ent in doc_obj.ents:\n",
    "                    if ent.label_ in q_type:\n",
    "                        filtered_sentences.append(sent)\n",
    "        else:\n",
    "            filtered_sentences = sentences\n",
    "\n",
    "        # compute cosine similarity of question with each sentence\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer=\"char\")\n",
    "        for sent in filtered_sentences:\n",
    "            compared_docs = (q,sent) \n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(compared_docs)\n",
    "            cs = cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)\n",
    "            if cs[0][1] > best_cs:\n",
    "                best_cs = cs[0][1]\n",
    "                best_sent = sent\n",
    "                \n",
    "    # extract answers from best sentence\n",
    "    answers = []\n",
    "    if len(q_type) and q_type[0] != 'NN':\n",
    "        best_obj = nlp(best_sent)\n",
    "        for ent in best_obj.ents:\n",
    "            if ent.label_ in q_type:\n",
    "                answers.append(ent.text)\n",
    "    elif q_type[0] == 'NN':\n",
    "        tags = nltk.pos_tag(word_tokenize(best_sent))\n",
    "        for tag in tags:\n",
    "            if 'NN' in tag[1]:\n",
    "                answers.append(tag[0])\n",
    "    else:\n",
    "        answers.append(best_sent)\n",
    "        \n",
    "    # sentence is also returned for debugging purposes\n",
    "    return answers, best_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla :\n",
      "['Elon Musk', 'Tesla']\n",
      "Elon Musk is CEO of Tesla and chairman of Solar City, so he's been shedding net worth.�How much net worth?\n",
      "\n",
      "Apple :\n",
      "['Tim Cook', 'Einhorn']\n",
      "Apple CEO Tim Cook has said he would take a look at Einhorn's proposal.\n",
      "\n",
      "Berkshire Hathaway :\n",
      "['Buffett']\n",
      "Sharp drops in many of the stocks owned by Buffett's Berkshire Hathaway in recent weeks hit the sprawling conglomerate's equity portfolio hard.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for company in [\"Tesla\",\"Apple\",\"Berkshire Hathaway\"]:    \n",
    "    docs = []\n",
    "    q = \"Who is the CEO of company \" + company + \"?\"\n",
    "    for idx in okapi_scoring(q,tokenized_texts,hashmap):\n",
    "        docs.append(all_text[idx])\n",
    "    print(company,\":\")\n",
    "    answer,sent = sentence_select(q,docs)\n",
    "    print(answer)\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "August of 2013 :\n",
      "(['Icahn', 'Tropicana Casino', 'Resort'], \"Icahn also owns Atlantic City's Tropicana Casino and Resort, which he bought out of bankruptcy in 2010.\")\n",
      "\n",
      "January of 2014 :\n",
      "(['McPhail', 'Superconductor', 'Superconductor', 'AMSC'], 'McPhail tipped the various defendants on other occasions, funneling them inside information about American Superconductor�s quarterly earnings announcements in July and September 2009, and again in January 2010.� He also alerted them in the fall of 2009 to a contract worth $100 million, and in November 2010 to a likely drop in American Superconductor�s share�s price, which occurred a few days later when AMSC announced a secondary stock offering.')\n",
      "\n",
      "May of 2014 :\n",
      "(['Ackman', 'Herbalife', 'FTC', 'Ackman'], 'Ackman said his bet against Herbalife is notionally larger than it was when he first initiated his short on the company, and if the company went out of business today, Ackman would make about $2 billion.� In March, the�FTC launched a formal investigation�into the company, which Ackman called a \"pyramid scheme\" in his presentation.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for date in [\"August of 2013\",\"January of 2014\",\"May of 2014\"]:\n",
    "    docs = []\n",
    "    q = \"Which companies went bankrupt in \" + date + \"?\"\n",
    "    for idx in okapi_scoring(q,tokenized_texts,hashmap):\n",
    "        docs.append(all_text[idx])\n",
    "    print(date,\":\")\n",
    "    answer,sent = sentence_select(q,docs)\n",
    "    print(answer)\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['end', 'Pascal', 'Wager'],\n",
       " \"What we forget at the end of all this is Pascal's Wager.\")"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "for idx in okapi_scoring(\"What affects GDP?\",tokenized_texts,hashmap):\n",
    "    docs.append(all_text[idx])\n",
    "    \n",
    "sentence_select(\"What affects GDP?\",docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unemployment :\n",
      "['3 percent']\n",
      "Economists say a growth pace in excess of 3 percent would be needed over a sustained period to significantly lower high unemployment.\n",
      "\n",
      "interest rates :\n",
      "['37 percent', '24 percent']\n",
      "Looking ahead, 37 percent of respondents anticipate further increases in raw materials prices over the next six months, while 24 percent expect higher finished goods prices.\n",
      "\n",
      "poverty :\n",
      "['0.8 percent']\n",
      "Weakness in German industrial output and both domestic and foreign orders have pointed to a poor April-June period after 0.8 percent expansion in the first three months of the year, the fastest rate in three years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prop in ['unemployment','interest rates','poverty']:\n",
    "    docs = []\n",
    "    q = \"What percentage of drop or increase in GDP is associated with \" + prop + \"?\"\n",
    "    for idx in okapi_scoring(q,tokenized_texts,hashmap):\n",
    "        docs.append(all_text[idx])\n",
    "    print(prop,\":\")\n",
    "    answer,sent = sentence_select(q,docs)\n",
    "    print(answer)\n",
    "    print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
